{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#要インストールライブラリ一覧"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#twitter api無しでtweet取得できる twint / pip install twintでインストールしても、ツイート取得ができない。\n",
    "!pip install  git+https://github.com/himanshudabas/twint.git@origin/master#egg=twint\n",
    "#gitインストール要\n",
    "\n",
    "#自然言語を可視化、分析できるライブラリ。\n",
    "!pip install nlplot\n",
    "\n",
    "!pip install janome\n",
    "!pip install MeCab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Twint　Twitter API不要でツイート取得が可能。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#twint 基本的使い方\n",
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure\n",
    "c = twint.Config()\n",
    "#特定のユーザー名\n",
    "c.Username = \"twitter\"\n",
    "#実行日から数えて過去何日分のツイートを取得するか\n",
    "c.Limit = 30\n",
    "#csv形式で保存するか\n",
    "c.Store_csv = True\n",
    "#保存ファイルの名前\n",
    "#c.Output = \"Sample.csv\"\n",
    "\n",
    "# Run\n",
    "twint.run.Search(c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Tweet取得して、差分確認して、csvへ保存していくコード・・・・らしい。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import twint\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    " \n",
    "ORG_CSV_PATH = \"to_csv_out.csv\"\n",
    "TMP_CSV_PATH = \"tmp.csv\"\n",
    " \n",
    "def csv_merge():\n",
    " \n",
    "    with open('test.csv','w') as n:\n",
    "         f = open(ORG_CSV_PATH, 'r')\n",
    "         reader = csv.reader(f)\n",
    "         header = next(reader)\n",
    " \n",
    "         m = open(TMP_CSV_PATH, 'r')\n",
    "         tmp_reader = csv.reader(m)\n",
    "         writer = csv.writer(n)\n",
    "         for row in tmp_reader:\n",
    "             writer.writerow(row)\n",
    "         for row in reader:\n",
    "             writer.writerow(row)\n",
    "         m.close()\n",
    "         f.close()\n",
    "    shutil.move(\"test.csv\", ORG_CSV_PATH)\n",
    " \n",
    "def fetch_tweet(query):\n",
    " \n",
    "    today = datetime.today()\n",
    "    if (os.path.isfile(ORG_CSV_PATH)):\n",
    "        since = today - timedelta(days=1)\n",
    "    else:\n",
    "        print(\"new csv\")\n",
    "        # 7 years\n",
    "        since = today - timedelta(weeks=364)\n",
    " \n",
    "    c = twint.Config()\n",
    "    c.Search = query\n",
    "    c.Store_csv = True\n",
    "    c.Output = TMP_CSV_PATH\n",
    "    c.Since = datetime.strftime(since, '%Y-%m-%d')\n",
    "    twint.run.Search(c)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "   fetch_tweet(\"uguisu.skr.jp -bot -recollection\")\n",
    "   csv_merge()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#保留コード類"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "for i,moji in enumerate(stop_list):\n",
    "    STOPWORDS.add(moji)\n",
    "\n",
    "for i,data in enumerate(Tweets_df.loc[:,\"tweet\"]):\n",
    "    text = text + data + \"\"\n",
    "\n",
    "stop = list(STOPWORDS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#display(Tweets_df)\n",
    "#print(a)\n",
    "\n",
    "text = \" \"\n",
    "for i,txt in enumerate(a):\n",
    "    text = text + \" \" + txt + \"\\n\"\n",
    "\n",
    "print(type(text))\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer\n",
    "\n",
    "for token in t.tokenize(text):\n",
    "    print(token)\n",
    "\n",
    "#wakati = [i for i in t.tokenize(text)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\n",
    "import re\n",
    "import MeCab\n",
    "\n",
    "def word_extraction(text):\n",
    "    \"\"\"文を取得して特定名詞だけ抽出する関数\"\"\"\n",
    "\n",
    "    #tagger = MeCab.Tagger('-d \"./mecab-ipadic-neologd/neologd\"')\n",
    "    #邪魔なURL部分を正規表現で排除\n",
    "    result = re.sub(\"https?://[\\w!\\?/\\+\\-_~=;\\.,\\*&@#\\$%\\(\\)'\\[\\]]+\", \"\", text)\n",
    "    parse = MeCab.parse(result) #解析\n",
    "\n",
    "    lines = parse.split('\\n') #改行で区切る\n",
    "    nounlist = [] #特定名詞を格納するリスト\n",
    "    for line in lines:\n",
    "        feature = line.split('\\t') #タブで区切る\n",
    "        if len(feature) == 2: #'EOS'と''を省く\n",
    "            info = feature[1].split(',') #カンマで区切る\n",
    "            hinshi = info[0] #その0番目(品詞)を取得\n",
    "            hinshi_classi = info[1] #1番目(品詞分類)を取得\n",
    "            #名詞 - 一般,サ変,固有名詞　だけ抽出する　※抽出したいものに合わせてカスタマイズすればいい\n",
    "            if (hinshi in ('名詞')) and ((hinshi_classi in ('一般')) or (hinshi_classi in ('サ変接続')) or (hinshi_classi in ('固有名詞'))):\n",
    "                nounlist.append(feature[0]) #info[6]を取得してもいいが、今回は原文を取得\n",
    "    return nounlist\n",
    "\n",
    "a = list(Tweets_df.loc[: , \"tweet\"])\n",
    "word_extraction(text)\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}