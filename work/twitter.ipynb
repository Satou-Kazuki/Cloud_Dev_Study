{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install  git+https://github.com/himanshudabas/twint.git@origin/master#egg=twint\n",
    "#gitインストール要\n",
    "\n",
    "!pip install nlplot\n",
    "!pip install janome\n",
    "!pip install MeCab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure\n",
    "c = twint.Config()\n",
    "#特定のユーザー名\n",
    "c.Username = \"twitter\"\n",
    "#実行日から数えて過去何日分のツイートを取得するか\n",
    "c.Limit = 30\n",
    "#csv形式で保存するか\n",
    "c.Store_csv = True\n",
    "#保存ファイルの名前\n",
    "#c.Output = \"Sample.csv\"\n",
    "\n",
    "# Run\n",
    "twint.run.Search(c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1438167531164999687 2021-09-15 15:47:13 +0000 <Twitter> @imOKcozImJK don't be shocked if we scream about BTS in your DMs\n",
      "1438165019263488008 2021-09-15 15:37:15 +0000 <Twitter> @NhatHoangRBLX on our way to wrestling Twitter\n",
      "1438161789557035010 2021-09-15 15:24:25 +0000 <Twitter> @djarinculture first is the worst second is the best third is the one with the treasure chest\n",
      "1438161655247089669 2021-09-15 15:23:53 +0000 <Twitter> @bre_naa following you because you're extraordinary\n",
      "1438159860416253957 2021-09-15 15:16:45 +0000 <Twitter> @ImLiteralLarry1 you're also Larry\n",
      "1438156066634747911 2021-09-15 15:01:40 +0000 <Twitter> @arikeokinn excited to be follower 543\n",
      "1438153955117305860 2021-09-15 14:53:17 +0000 <Twitter> @hedasripa you could've just told us you'd put more carol danvers on our timeline\n",
      "1438151307643260931 2021-09-15 14:42:45 +0000 <Twitter> @jakehue_ NEED this on our timeline\n",
      "1438149367005843462 2021-09-15 14:35:03 +0000 <Twitter> @louanben where would we be without kissing-heart + in-tears\n",
      "1438147153537421318 2021-09-15 14:26:15 +0000 <Twitter> @brenduhalvarado AND you rhymed. definitely makes sense\n",
      "1438145523853053955 2021-09-15 14:19:46 +0000 <Twitter> ok but why should we follow you\n",
      "1437517089321930754 2021-09-13 20:42:36 +0000 <Twitter> @ashhhhhhole so how did it go\n",
      "1437482852795437062 2021-09-13 18:26:33 +0000 <Twitter> @iBYGLOSS make the periods a bigger font than the rest of the essay if you're worried about page count\n",
      "1437482819253637129 2021-09-13 18:26:25 +0000 <Twitter> @lunrwave SOHCAHTOA\n",
      "1437482642753179656 2021-09-13 18:25:43 +0000 <Twitter> @SilviaCorreia05 need to learn from you\n",
      "1437481121663660037 2021-09-13 18:19:41 +0000 <Twitter> @GretaAilie definitely reading our old Tweets\n",
      "1437481109349093381 2021-09-13 18:19:38 +0000 <Twitter> @kapp_cap we can help if it's easy math\n",
      "1437480160098463744 2021-09-13 18:15:51 +0000 <Twitter> @majodk10 oops\n",
      "1437480000442277894 2021-09-13 18:15:13 +0000 <Twitter> @grzmot69 then you're in the right place\n",
      "1437479366003417095 2021-09-13 18:12:42 +0000 <Twitter> @auroravua1 sorry for being rude\n",
      "1437479328229568516 2021-09-13 18:12:33 +0000 <Twitter> @its_menieb learn something fun\n",
      "1437478429557932035 2021-09-13 18:08:59 +0000 <Twitter> hello to everyone reading this while you should be doing something else\n",
      "1435996693300928515 2021-09-09 16:01:05 +0000 <Twitter> @419OO that's aries sun, virgo rising to you\n",
      "1435995816943099904 2021-09-09 15:57:36 +0000 <Twitter> @96138 they're perfect AND self-aware\n",
      "1435995800455233536 2021-09-09 15:57:32 +0000 <Twitter> @Karasmyoui so you're calling an aries sus now?\n",
      "1435993082311417864 2021-09-09 15:46:44 +0000 <Twitter> @ftblabbas yeah it was in our horoscope this week\n",
      "1435992847212294155 2021-09-09 15:45:48 +0000 <Twitter> @Jinieseokim yup and we're not afraid to say it\n",
      "1435992418084663301 2021-09-09 15:44:06 +0000 <Twitter> @shfaffler sorry but it's virgo season rn\n",
      "1435992401055821847 2021-09-09 15:44:02 +0000 <Twitter> @LynxyFynxy you must identify with your rising\n",
      "1435990839013126149 2021-09-09 15:37:50 +0000 <Twitter> virgos don’t need an edit button\n",
      "1435987777028493313 2021-09-09 15:25:40 +0000 <Twitter> @adviceburner  https://t.co/hoYD09Impn\n",
      "1433466824880971780 2021-09-02 16:28:18 +0000 <Twitter> @FatihAkTurko can't think about that yet\n",
      "1433466520970186761 2021-09-02 16:27:05 +0000 <Twitter> @KirtReyes the kind that Tweets\n",
      "1433465847037796358 2021-09-02 16:24:25 +0000 <Twitter> @elleseh can we hoot for october instead\n",
      "1433465586911166471 2021-09-02 16:23:23 +0000 <Twitter> @_shawkay the only way to put it tbh\n",
      "1433465431059271688 2021-09-02 16:22:45 +0000 <Twitter> @lilyoongi_07 bestie, i'm afraid to tell you this...\n",
      "1433465220618412033 2021-09-02 16:21:55 +0000 <Twitter> @polaqwym let's skip to then\n",
      "1433462575438643206 2021-09-02 16:11:25 +0000 <Twitter> we were rooting for you, september\n",
      "1430622702499414016 2021-08-25 20:06:46 +0000 <Twitter> @mollyhannahm tell your professor Twitter replied\n",
      "1425859813909667843 2021-08-12 16:40:45 +0000 <Twitter> @playboixcarson give them a week, their taste has to develop\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "jst_tz = timezone(timedelta(hours=9), 'JST')\n",
    "\n",
    "jh = timedelta(hours=9)\n",
    "thb = timedelta(hours=1)\n",
    "utc = datetime.now()\n",
    "jst = utc + jh\n",
    "before_1h = jst - thb\n",
    "print(thb)\n",
    "now_str = jst.strftime('%Y-%m-%d %H:%M')\n",
    "before_1h_str = before_1h.strftime('%Y-%m-%d %H:%M')\n",
    "print(now_str,before_1h_str)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1:00:00\n",
      "2021-09-20 20:08 2021-09-20 19:08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#これないとエラーが出る・・・・\n",
    "nest_asyncio.apply()\n",
    "\n",
    "with open('user.txt', 'r') as f:\n",
    "    user_list = f.read().split(\",\")\n",
    "\n",
    "#hako_df = pd.DataFrame()\n",
    "output_filename = \"twint_scrape2.csv\"\n",
    "\n",
    "for i,twi in enumerate(user_list):\n",
    "    try:\n",
    "        c = twint.Config()\n",
    "        c.Username = twi#解析するユーザー名\n",
    "        #c.Store_json = True # Store_csvもあるがカラム名が入らなかったため不採用\n",
    "        c.Store_csv = True\n",
    "        c.Output = output_filename #上記アウトプットするJSONファイルの名前\n",
    "        c.Since = before_1h_str#\"2019-01-01 00:00:00\" #ここから\n",
    "        c.Until = now_str#\"2021-09-01 00:00:00\" #ここまで\n",
    "        #c.Limit = 1\n",
    "        #c.Pandas = True #出力形式をPandasにする\n",
    "        c.Hide_output = False # False にするとツイートがターミナルに出力\n",
    "        twint.run.Search(c)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        #df = twint.storage.panda.Tweets_df #データフレーム形式へ変換\n",
    "        #Tweets_df = df['tweet'].to_frame() #tweetカラムのみデータフレーム形式で抽出\n",
    "        #hako_df = pd.concat([hako_df, Tweets_df], axis=0)\n",
    "'''\n",
    "tweets_list = Tweets_df['tweet'].tolist() #tweetカラムをlist化して抽出しておく\n",
    "a = list(Tweets_df.loc[: , \"tweet\"])\n",
    "display(Tweets_df.loc[: , \"tweet\"])\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntweets_list = Tweets_df[\\'tweet\\'].tolist() #tweetカラムをlist化して抽出しておく\\na = list(Tweets_df.loc[: , \"tweet\"])\\ndisplay(Tweets_df.loc[: , \"tweet\"])\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twint_scrape.csv', encoding=\"utf-8\")\n",
    "Tweets_df = df.loc[: , \"tweet\"]\n",
    "#display(Tweets_df)\n",
    "with open('stop.txt', 'r') as f:\n",
    "    stop_list = f.read().split(\",\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "from janome.charfilter import *\n",
    "\n",
    "#形態素解析器の設定\n",
    "char_filters = [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r\"[IiⅠｉ?.*/~=()〝 <>:：《°!！!？（）-]+\", \"\")]\n",
    "t = Tokenizer()\n",
    "token_filters = [POSKeepFilter([\"名詞\"]), POSStopFilter([\"名詞,非自立\", \"名詞,数\", \"名詞,代名詞\", \"名詞,接尾\"]),LowerCaseFilter()]\n",
    "analyzer = Analyzer(char_filters=char_filters, token_filters=token_filters)\n",
    "\n",
    "Tweets_df = Tweets_df.apply(lambda x: \" \".join([token.surface for token in analyzer.analyze(x)]))\n",
    "Tweets_df = pd.DataFrame(Tweets_df)\n",
    "text = \"\"\n",
    "for i,data in enumerate(Tweets_df.loc[:,\"tweet\"]):\n",
    "    text = text + data + \"\"\n",
    "\n",
    "word_txt = {}\n",
    "lines = text.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    malist = t.tokenize(line)\n",
    "    for w in malist:\n",
    "        word = w.surface#4---単語情報の読込\n",
    "        ps = w.part_of_speech #5---品詞情報の読込\n",
    "        if ps.find('名詞') < 0: continue #6---名詞のカウント\n",
    "        if not word in word_txt:\n",
    "            word_txt[word] = 0\n",
    "        word_txt[word] += 1 #7---カウント\n",
    "#8---頻出単語の表示\n",
    "keys = sorted(word_txt.items(), key=lambda x:x[1], reverse=True)\n",
    "print(type(keys))\n",
    "\n",
    "for word,cnt in keys:\n",
    "    if any([word == i for i in stop_list]):\n",
    "        continue\n",
    "    elif word == \",\":\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{0}({1}) \".format(word,cnt), end=\"\")\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "for i,moji in enumerate(stop_list):\n",
    "    STOPWORDS.add(moji)\n",
    "\n",
    "for i,data in enumerate(Tweets_df.loc[:,\"tweet\"]):\n",
    "    text = text + data + \"\"\n",
    "\n",
    "stop = list(STOPWORDS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import twint\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    " \n",
    "ORG_CSV_PATH = \"to_csv_out.csv\"\n",
    "TMP_CSV_PATH = \"tmp.csv\"\n",
    " \n",
    "def csv_merge():\n",
    " \n",
    "    with open('test.csv','w') as n:\n",
    "         f = open(ORG_CSV_PATH, 'r')\n",
    "         reader = csv.reader(f)\n",
    "         header = next(reader)\n",
    " \n",
    "         m = open(TMP_CSV_PATH, 'r')\n",
    "         tmp_reader = csv.reader(m)\n",
    "         writer = csv.writer(n)\n",
    "         for row in tmp_reader:\n",
    "             writer.writerow(row)\n",
    "         for row in reader:\n",
    "             writer.writerow(row)\n",
    "         m.close()\n",
    "         f.close()\n",
    "    shutil.move(\"test.csv\", ORG_CSV_PATH)\n",
    " \n",
    "def fetch_tweet(query):\n",
    " \n",
    "    today = datetime.today()\n",
    "    if (os.path.isfile(ORG_CSV_PATH)):\n",
    "        since = today - timedelta(days=1)\n",
    "    else:\n",
    "        print(\"new csv\")\n",
    "        # 7 years\n",
    "        since = today - timedelta(weeks=364)\n",
    " \n",
    "    c = twint.Config()\n",
    "    c.Search = query\n",
    "    c.Store_csv = True\n",
    "    c.Output = TMP_CSV_PATH\n",
    "    c.Since = datetime.strftime(since, '%Y-%m-%d')\n",
    "    twint.run.Search(c)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "   fetch_tweet(\"uguisu.skr.jp -bot -recollection\")\n",
    "   csv_merge()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#自然言語を簡単に可視化・分析できるライブラリ\n",
    "import nlplot\n",
    "\n",
    "npt = nlplot.NLPlot(Tweets_df, target_col='tweet')\n",
    "\n",
    "stopwords = npt.get_stopword(top_n=10, min_freq=0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#N-gram bar chart\n",
    "npt.bar_ngram(\n",
    "    title='uni-gram',\n",
    "    xaxis_label='word_count',\n",
    "    yaxis_label='word',\n",
    "    ngram=1,\n",
    "    top_n=50,\n",
    "    stopwords=stop,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#N-gram tree Map\n",
    "npt.treemap(\n",
    "    title='Tree of Most Common Words',\n",
    "    ngram=1,\n",
    "    top_n=30,\n",
    "    stopwords=stop,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Histogram of the word count\n",
    "npt.word_distribution(\n",
    "    title='number of words distribution',\n",
    "    xaxis_label='count',\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "npt.wordcloud(\n",
    "    max_words=300,\n",
    "    max_font_size=100,\n",
    "    colormap='tab20_r',\n",
    "    stopwords=stop,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#co-occurrence networks\n",
    "# ビルド（データ件数によっては処理に時間を要します）※ノードの数のみ変更\n",
    "npt.build_graph(stopwords=stopwords, min_edge_frequency=1)\n",
    "\n",
    "display(\n",
    "    npt.node_df.head(), npt.node_df.shape,\n",
    "    npt.edge_df.head(), npt.edge_df.shape\n",
    ")\n",
    "\n",
    "npt.co_network(\n",
    "    title='Co-occurrence network',\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#display(Tweets_df)\n",
    "#print(a)\n",
    "\n",
    "text = \" \"\n",
    "for i,txt in enumerate(a):\n",
    "    text = text + \" \" + txt + \"\\n\"\n",
    "\n",
    "print(type(text))\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer\n",
    "\n",
    "for token in t.tokenize(text):\n",
    "    print(token)\n",
    "\n",
    "#wakati = [i for i in t.tokenize(text)]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\n",
    "import re\n",
    "import MeCab\n",
    "\n",
    "def word_extraction(text):\n",
    "    \"\"\"文を取得して特定名詞だけ抽出する関数\"\"\"\n",
    "\n",
    "    #tagger = MeCab.Tagger('-d \"./mecab-ipadic-neologd/neologd\"')\n",
    "    #邪魔なURL部分を正規表現で排除\n",
    "    result = re.sub(\"https?://[\\w!\\?/\\+\\-_~=;\\.,\\*&@#\\$%\\(\\)'\\[\\]]+\", \"\", text)\n",
    "    parse = MeCab.parse(result) #解析\n",
    "\n",
    "    lines = parse.split('\\n') #改行で区切る\n",
    "    nounlist = [] #特定名詞を格納するリスト\n",
    "    for line in lines:\n",
    "        feature = line.split('\\t') #タブで区切る\n",
    "        if len(feature) == 2: #'EOS'と''を省く\n",
    "            info = feature[1].split(',') #カンマで区切る\n",
    "            hinshi = info[0] #その0番目(品詞)を取得\n",
    "            hinshi_classi = info[1] #1番目(品詞分類)を取得\n",
    "            #名詞 - 一般,サ変,固有名詞　だけ抽出する　※抽出したいものに合わせてカスタマイズすればいい\n",
    "            if (hinshi in ('名詞')) and ((hinshi_classi in ('一般')) or (hinshi_classi in ('サ変接続')) or (hinshi_classi in ('固有名詞'))):\n",
    "                nounlist.append(feature[0]) #info[6]を取得してもいいが、今回は原文を取得\n",
    "    return nounlist\n",
    "\n",
    "a = list(Tweets_df.loc[: , \"tweet\"])\n",
    "word_extraction(text)\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}