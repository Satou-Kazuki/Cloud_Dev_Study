{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install  git+https://github.com/himanshudabas/twint.git@origin/master#egg=twint\n",
    "#gitインストール要\n",
    "\n",
    "!pip install nlplot\n",
    "!pip install janome\n",
    "!conda install MeCab #pipだとうまくいかなかった・・・"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure\n",
    "c = twint.Config()\n",
    "#特定のユーザー名\n",
    "c.Username = \"twitter\"\n",
    "#実行日から数えて過去何日分のツイートを取得するか\n",
    "c.Limit = 30\n",
    "#csv形式で保存するか\n",
    "c.Store_csv = True\n",
    "#保存ファイルの名前\n",
    "#c.Output = \"Sample.csv\"\n",
    "\n",
    "# Run\n",
    "twint.run.Search(c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "#これないとエラーが出る・・・・\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def Twitter_Scraper(user_name,since_date,until_date,output_filename):\n",
    "    c = twint.Config()\n",
    "    c.Username = user_name #解析するユーザー名\n",
    "    c.Store_json = True # Store_csvもあるがカラム名が入らなかったため不採用\n",
    "    c.Output = output_filename #上記アウトプットするJSONファイルの名前\n",
    "    c.Since = since_date #ここから\n",
    "    c.Until = until_date #ここまで\n",
    "    c.Pandas = True #出力形式をPandasにする\n",
    "    c.Hide_output = True # False にするとツイートがターミナルに出力\n",
    "    twint.run.Search(c)\n",
    "\n",
    "output_file = \"twint_scrape.json\"\n",
    "Twitter_Scraper(\"Qiita\", \"2020-09-18 00:00:00\", \"2021-09-18 13:00:00\", output_file)\n",
    "\n",
    "df = twint.storage.panda.Tweets_df #データフレーム形式へ変換\n",
    "Tweets_df = df['tweet'].to_frame() #tweetカラムのみデータフレーム形式で抽出\n",
    "tweets_list = Tweets_df['tweet'].tolist() #tweetカラムをlist化して抽出しておく"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "def word_extraction(text):\n",
    "    \"\"\"文を取得して特定名詞だけ抽出する関数\"\"\"\n",
    "\n",
    "    #邪魔なURL部分を正規表現で排除\n",
    "    result = re.sub(\"https?://[\\w!\\?/\\+\\-_~=;\\.,\\*&@#\\$%\\(\\)'\\[\\]]+\", \"\", text)\n",
    "    parse = tagger.parse(result) #解析\n",
    "\n",
    "    lines = parse.split('\\n') #改行で区切る\n",
    "    nounlist = [] #特定名詞を格納するリスト\n",
    "    for line in lines:\n",
    "        feature = line.split('\\t') #タブで区切る\n",
    "        if len(feature) == 2: #'EOS'と''を省く\n",
    "            info = feature[1].split(',') #カンマで区切る\n",
    "            hinshi = info[0] #その0番目(品詞)を取得\n",
    "            hinshi_classi = info[1] #1番目(品詞分類)を取得\n",
    "            #名詞 - 一般,サ変,固有名詞　だけ抽出する　※抽出したいものに合わせてカスタマイズすればいい\n",
    "            if (hinshi in ('名詞')) and ((hinshi_classi in ('一般')) or (hinshi_classi in ('サ変接続')) or (hinshi_classi in ('固有名詞'))):\n",
    "                nounlist.append(feature[0]) #info[6]を取得してもいいが、今回は原文を取得\n",
    "    return nounlist\n",
    "\n",
    "#wordsカラムを↑の関数で挿入する\n",
    "Tweets_df['words'] = Tweets_df['tweet'].apply(word_extraction)\n",
    "\n",
    "Tweets_df.head(1) #1行表示する\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MeCab'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24785/2983443227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"文を取得して特定名詞だけ抽出する関数\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MeCab'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}